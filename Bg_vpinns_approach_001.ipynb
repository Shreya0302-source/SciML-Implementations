{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 93864,
          "databundleVersionId": 11172267,
          "sourceType": "competition"
        },
        {
          "sourceId": 10845578,
          "sourceType": "datasetVersion",
          "datasetId": 6735650
        }
      ],
      "dockerImageVersionId": 30918,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shreya0302-source/SciML-Implementations/blob/main/Bg_vpinns_approach_001.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-26T17:29:35.807208Z",
          "iopub.execute_input": "2025-02-26T17:29:35.807605Z",
          "iopub.status.idle": "2025-02-26T17:29:35.81274Z",
          "shell.execute_reply.started": "2025-02-26T17:29:35.807576Z",
          "shell.execute_reply": "2025-02-26T17:29:35.811592Z"
        },
        "id": "70nqcMhCJv8k"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the thermal diffusion equation: -epsilon * (u_xx + u_yy) + bx * u_x + by * u_y = f(x, y)\n",
        "def convection_diffusion_equation(x, y, u, u_xx, u_yy, u_x, u_y, epsilon, bx, by):\n",
        "    return -epsilon * (u_xx + u_yy) + bx * u_x + by * u_y"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-26T17:29:35.814191Z",
          "iopub.execute_input": "2025-02-26T17:29:35.814554Z",
          "iopub.status.idle": "2025-02-26T17:29:35.830523Z",
          "shell.execute_reply.started": "2025-02-26T17:29:35.814526Z",
          "shell.execute_reply": "2025-02-26T17:29:35.829166Z"
        },
        "id": "bNYfpW1lJv8l"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Dirichlet boundary condition for temperature\n",
        "def temperature_bc(x):\n",
        "    return tf.zeros_like(tf.expand_dims(x[:, 0], axis=1))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-26T17:29:35.832611Z",
          "execution_failed": "2025-02-26T17:30:32.309Z"
        },
        "id": "tnHcqL9_Jv8l"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the input data\n",
        "def normalize_data(data, xmin, xmax, ymin, ymax):\n",
        "    data[:, 0] = (data[:, 0] - xmin) / (xmax - xmin)\n",
        "    data[:, 1] = (data[:, 1] - ymin) / (ymax - ymin)\n",
        "    return data"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-02-26T17:30:32.309Z"
        },
        "id": "GkPgINtgJv8l"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_derivatives(model, x, y):\n",
        "    with tf.GradientTape(persistent=True) as tape1:\n",
        "        tape1.watch(x)\n",
        "        tape1.watch(y)\n",
        "        with tf.GradientTape(persistent=True) as tape2:\n",
        "            tape2.watch(x)\n",
        "            tape2.watch(y)\n",
        "            u = model(tf.concat([x, y], axis=1))\n",
        "        u_x = tape2.gradient(u, x)\n",
        "        u_y = tape2.gradient(u, y)\n",
        "    u_xx = tape1.gradient(u_x, x)\n",
        "    u_yy = tape1.gradient(u_y, y)\n",
        "    del tape1, tape2\n",
        "    return u, u_x, u_y, u_xx, u_yy"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-02-26T17:30:32.309Z"
        },
        "id": "IqGi-KrkJv8l"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the forcing function for thermal diffusion\n",
        "@tf.function\n",
        "def convection_diffusion_forcing_function(x, y, eps):\n",
        "    return (2 * eps * (-x + tf.exp(2 * (x - 1) * eps))\n",
        "            + x * y**2 + 6 * x * y\n",
        "            - x * tf.exp(3 * (y - 1) * eps)\n",
        "            - y**2 * tf.exp(2 * (x - 1) * eps)\n",
        "            + 2 * y**2 - 6 * y * tf.exp(2 * (x - 1) * eps)\n",
        "            - 2 * tf.exp(3 * (y - 1) * eps)\n",
        "            + tf.exp(2*x + 3*y - 5*eps))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-02-26T17:30:32.309Z"
        },
        "id": "te07Z8FrJv8m"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the differential equation: u_xx(x, y) + u_yy(x, y) + u(x, y) = 0\n",
        "def differential_equation(x, y, u, u_xx, u_yy):\n",
        "    return u_xx + u_yy + u"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-02-26T17:30:32.309Z"
        },
        "id": "QuZampnFJv8m"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the neural network model\n",
        "class VPINN(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(VPINN, self).__init__()\n",
        "        self.input_layer = tf.keras.layers.InputLayer(shape=(2,))\n",
        "        self.hidden_layer_1 = tf.keras.layers.Dense(\n",
        "            50,  # Increased number of neurons\n",
        "            activation=tf.nn.tanh,\n",
        "            kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
        "        )\n",
        "        self.batch_norm_1 = tf.keras.layers.BatchNormalization()\n",
        "        self.hidden_layer_2 = tf.keras.layers.Dense(\n",
        "            50,  # Increased number of neurons\n",
        "            activation=tf.nn.tanh,\n",
        "            kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
        "        )\n",
        "        self.batch_norm_2 = tf.keras.layers.BatchNormalization()\n",
        "        self.hidden_layer_3 = tf.keras.layers.Dense(\n",
        "            50,  # Increased number of neurons\n",
        "            activation=tf.nn.tanh,\n",
        "            kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
        "        )\n",
        "        self.batch_norm_3 = tf.keras.layers.BatchNormalization()\n",
        "        self.hidden_layer_4 = tf.keras.layers.Dense(\n",
        "            50,  # Added an additional hidden layer\n",
        "            activation=tf.nn.tanh,\n",
        "            kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
        "        )\n",
        "        self.batch_norm_4 = tf.keras.layers.BatchNormalization()\n",
        "        self.output_layer = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.hidden_layer_1(inputs)\n",
        "        x = self.batch_norm_1(x)\n",
        "        x = self.hidden_layer_2(x)\n",
        "        x = self.batch_norm_2(x)\n",
        "        x = self.hidden_layer_3(x)\n",
        "        x = self.batch_norm_3(x)\n",
        "        x = self.hidden_layer_4(x)\n",
        "        x = self.batch_norm_4(x)\n",
        "        return self.output_layer(x)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-02-26T17:30:32.309Z"
        },
        "id": "TKuqrrC6Jv8m"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the PDE convection loss function with RMSE\n",
        "def pde_loss(model, train_interior, epsilon, bx, by):\n",
        "    x = tf.expand_dims(train_interior[:, 0], axis=1)\n",
        "    y = tf.expand_dims(train_interior[:, 1], axis=1)\n",
        "\n",
        "    u, u_x, u_y, u_xx, u_yy = compute_derivatives(model, x, y)\n",
        "    residual = convection_diffusion_equation(x, y, u, u_xx, u_yy, u_x, u_y, epsilon, bx, by) - convection_diffusion_forcing_function(x, y, epsilon)\n",
        "    return tf.sqrt(tf.reduce_mean(tf.square(residual))) # RMSE between the residuals\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-02-26T17:30:32.31Z"
        },
        "id": "-4cXLKusJv8m"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def bc_loss(model, x_bd):\n",
        "    u_pred = model(x_bd)\n",
        "    u_exact = temperature_bc(x_bd)\n",
        "\n",
        "    # Calculate RMSE loss\n",
        "    # RMSE between predicted and exact boundary values\n",
        "    return tf.sqrt(tf.reduce_mean(tf.square(u_pred - u_exact)))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-02-26T17:30:32.31Z"
        },
        "id": "SRBiAuIBJv8n"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate boundary points\n",
        "def generate_boundary_points(xmin, xmax, ymin, ymax, num_points):\n",
        "    np.random.seed(42)  # For reproducibility\n",
        "    num_per_edge = num_points // 4\n",
        "    remainder = num_points % 4  # Handle remainder properly\n",
        "\n",
        "    bottom = np.column_stack((np.random.uniform(xmin, xmax, num_per_edge + (remainder > 0)), np.full((num_per_edge + (remainder > 0),), ymin)))\n",
        "    right = np.column_stack((np.full((num_per_edge + (remainder > 1),), xmax), np.random.uniform(ymin, ymax, num_per_edge + (remainder > 1))))\n",
        "    top = np.column_stack((np.random.uniform(xmin, xmax, num_per_edge + (remainder > 2)), np.full((num_per_edge + (remainder > 2),), ymax)))\n",
        "    left = np.column_stack((np.full((num_per_edge,), xmin), np.random.uniform(ymin, ymax, num_per_edge)))\n",
        "\n",
        "    boundary_points = np.vstack((bottom, right, top, left))\n",
        "\n",
        "    return tf.convert_to_tensor(boundary_points, dtype=tf.float32)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-02-26T17:30:32.31Z"
        },
        "id": "RnrunxDEJv8n"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_interior_points(xmin, xmax, ymin, ymax, num_points):\n",
        "    \"\"\"\n",
        "    Generate random interior points inside the domain (excluding boundaries).\n",
        "\n",
        "    Args:\n",
        "        xmin, xmax: Domain limits in x-direction\n",
        "        ymin, ymax: Domain limits in y-direction\n",
        "        num_points: Number of points to generate\n",
        "\n",
        "    Returns:\n",
        "        tf.Tensor: Interior points as a (num_points, 2) tensor\n",
        "    \"\"\"\n",
        "    np.random.seed(42)  # Ensures reproducibility\n",
        "    delta = 1e-5  # Small shift to prevent boundary points\n",
        "\n",
        "    x_interior = np.random.uniform(xmin + delta, xmax - delta, (num_points, 1))\n",
        "    y_interior = np.random.uniform(ymin + delta, ymax - delta, (num_points, 1))\n",
        "\n",
        "    interior_points = np.column_stack((x_interior, y_interior))\n",
        "\n",
        "    return tf.convert_to_tensor(interior_points, dtype=tf.float32)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-02-26T17:30:32.31Z"
        },
        "id": "PQCiqNBGJv8n"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate training data\n",
        "xmin, xmax, ymin, ymax = 0, 2 * np.pi, 0, 2 * np.pi\n",
        "num_boundary_points = 4000\n",
        "num_interior_points = 20000"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-02-26T17:30:32.31Z"
        },
        "id": "u7BbOMFHJv8n"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate boundary and interior points\n",
        "boundary_points = generate_boundary_points(xmin, xmax, ymin, ymax, num_boundary_points)\n",
        "interior_points = generate_interior_points(xmin, xmax, ymin, ymax, num_interior_points)\n",
        "\n",
        "# Normalize the training data\n",
        "boundary_points = normalize_data(boundary_points.numpy(), xmin, xmax, ymin, ymax)\n",
        "interior_points = normalize_data(interior_points.numpy(), xmin, xmax, ymin, ymax)\n",
        "\n",
        "# Convert training data to TensorFlow tensors\n",
        "boundary_points = tf.convert_to_tensor(boundary_points, dtype=tf.float32)\n",
        "interior_points = tf.convert_to_tensor(interior_points, dtype=tf.float32)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-02-26T17:30:32.31Z"
        },
        "id": "rQyTDhf1Jv8n"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the training step function\n",
        "# @tf.function\n",
        "# Adaptive training step with gradient clipping\n",
        "def train_step(model, optimizer, train_interior, train_boundary, epsilon, bx, by):\n",
        "    try:\n",
        "        with tf.GradientTape() as tape:\n",
        "            loss_pde = pde_loss(model, train_interior, epsilon, bx, by)\n",
        "            loss_bc = bc_loss(model, train_boundary)\n",
        "            total_loss = loss_pde + loss_bc\n",
        "        grads = tape.gradient(total_loss, model.trainable_variables)\n",
        "        grads, _ = tf.clip_by_global_norm(grads, 1.0)  # Gradient clipping for stability\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "        return total_loss, loss_pde, loss_bc\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in training step at epoch {epoch}: {str(e)}\")\n",
        "        return None, None, None  # Return None in case of an error"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-02-26T17:30:32.31Z"
        },
        "id": "qr7V3dmUJv8n"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def adjust_weights(alpha, beta, loss_pde, loss_bc, factor=1.05, max_value=10.0, min_value=0.1):\n",
        "    if loss_pde > loss_bc:\n",
        "        new_alpha = min(max(alpha / factor, min_value), max_value)\n",
        "        new_beta = min(max(beta * factor, min_value), max_value)\n",
        "    else:\n",
        "        new_alpha = min(max(alpha * factor, min_value), max_value)\n",
        "        new_beta = min(max(beta / factor, min_value), max_value)\n",
        "    return new_alpha, new_beta"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-02-26T17:30:32.31Z"
        },
        "id": "U9K0Dvr2Jv8n"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# SSBroyden optimizer implementation\n",
        "class SSBroydenOptimizer(tf.keras.optimizers.Optimizer):\n",
        "    def __init__(self, learning_rate=0.1, name=\"SSBroyden\", **kwargs):\n",
        "        super(SSBroydenOptimizer, self).__init__(name, **kwargs)\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def _resource_apply_dense(self, grad, var, apply_state=None):\n",
        "        var.assign_sub(self.learning_rate * grad)\n",
        "\n",
        "    def _resource_apply_sparse(self, grad, var, indices, apply_state=None):\n",
        "        var.scatter_sub(tf.IndexedSlices(self.learning_rate * grad, indices))"
      ],
      "metadata": {
        "id": "uw80jBn5PCy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model and optimizer with a learning rate scheduler\n",
        "model = VPINN()\n",
        "initial_learning_rate = 0.001\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate, decay_steps=1000, decay_rate=0.9, staircase=True)\n",
        "optimizer_1 = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "optimizer_2 = SSBroydenOptimizer(learning_rate=lr_schedule)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-02-26T17:30:32.31Z"
        },
        "id": "3xah6O_uJv8n"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize arrays to store loss values\n",
        "total_loss_ar = []\n",
        "pde_loss_ar = []\n",
        "bound_loss_ar = []\n",
        "\n",
        "# Initialize bx and by\n",
        "bx = 2.0\n",
        "by = 3.0\n",
        "\n",
        "# Convert training data to TensorFlow tensors\n",
        "train_boundary = tf.convert_to_tensor(boundary_points, dtype=tf.float32)\n",
        "train_interior = tf.convert_to_tensor(interior_points, dtype=tf.float32)\n",
        "\n",
        "# Initialize eps\n",
        "epsilon = 0.0001\n",
        "\n",
        "# Training loop with early stopping\n",
        "epochs = 20000\n",
        "patience = 1000  # Number of epochs to wait for improvement\n",
        "best_loss = float('inf')\n",
        "epochs_without_improvement = 0"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-02-26T17:30:32.31Z"
        },
        "id": "zZ1to44aJv8o"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "  if epoch <= 2000:\n",
        "    optimizer = optimizer_1\n",
        "  else:\n",
        "    optimizer = optimizer_2\n",
        "    # Perform a training step\n",
        "    total_loss, loss_pde, loss_bc = train_step(model, optimizer, interior_points, boundary_points, epsilon, bx, by)\n",
        "\n",
        "    # Store loss values\n",
        "    total_loss_ar.append(total_loss.numpy())\n",
        "    pde_loss_ar.append(loss_pde.numpy())\n",
        "    bound_loss_ar.append(loss_bc.numpy())\n",
        "\n",
        "    # Check for NaN values\n",
        "    if tf.math.is_nan(total_loss):\n",
        "        print(f'NaN encountered at epoch {epoch}')\n",
        "        break\n",
        "\n",
        "    # Check for improvement\n",
        "    if total_loss < best_loss:\n",
        "        best_loss = total_loss\n",
        "        epochs_without_improvement = 0\n",
        "    else:\n",
        "        epochs_without_improvement += 1\n",
        "\n",
        "    # Print progress\n",
        "    if epoch % 100 == 0:\n",
        "        print(f'Epoch {epoch}, PDE Loss: {loss_pde.numpy()}, BC Loss: {loss_bc.numpy()}, Total Loss: {total_loss.numpy()}')\n",
        "\n",
        "    # # Early stopping\n",
        "    # if epochs_without_improvement >= patience:\n",
        "    #     print(f'Stopping early at epoch {epoch}')\n",
        "    #     break"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-02-26T17:30:32.31Z"
        },
        "id": "OmReJacPJv8o"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the total loss, PDE loss, and boundary loss\n",
        "plt.figure()\n",
        "plt.yscale('log')\n",
        "plt.plot(total_loss_ar, label='Total Loss')\n",
        "plt.plot(pde_loss_ar, label='PDE Loss')\n",
        "plt.plot(bound_loss_ar, label='Boundary Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-02-26T17:30:32.311Z"
        },
        "id": "Fge9sLhdJv8o"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict and print results\n",
        "test_data = pd.read_csv(r'/kaggle/input/ziq-sciml-challenge/test.csv')\n",
        "x_test = tf.convert_to_tensor(test_data['x'].values.reshape(-1, 1).astype(np.float32))\n",
        "y_test = tf.convert_to_tensor(test_data['y'].values.reshape(-1, 1).astype(np.float32))\n",
        "\n",
        "# Concatenate test inputs and make predictions\n",
        "inputs_test = tf.concat([x_test, y_test], axis=1)\n",
        "u_pred = model(inputs_test)\n",
        "\n",
        "# Convert predictions to a NumPy array\n",
        "u_pred_np = u_pred.numpy()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-02-26T17:30:32.311Z"
        },
        "id": "3DGmv9tKJv8o"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame with the predictions\n",
        "submission_df = pd.DataFrame({\n",
        "    'ID': test_data['ID'],\n",
        "    'x': test_data['x'],\n",
        "    'y': test_data['y'],\n",
        "    'u': u_pred_np.flatten()\n",
        "})\n",
        "# Save the DataFrame to a CSV file\n",
        "submission_df.to_csv('/kaggle/working/submission.csv', index=False)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-02-26T17:30:32.311Z"
        },
        "id": "U_1JItYxJv8o"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}